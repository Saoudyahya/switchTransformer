{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzJp0nMP2eOTjg/yfTeVYj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saoudyahya/switchTransformer/blob/main/switchTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == hidden_size\n",
        "        ), \"hidden_size must be divisible by num_heads\"\n",
        "\n",
        "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        queries = self.q_linear(x)  # (batch_size, seq_length, hidden_size)\n",
        "        keys = self.k_linear(x)      # (batch_size, seq_length, hidden_size)\n",
        "        values = self.v_linear(x)    # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)          # (batch_size, num_heads, seq_length, head_dim)\n",
        "        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)    # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "        attn_weights = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "        output = torch.matmul(attn_weights, values)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Concatenate heads and pass through output linear layer\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)  # (batch_size, seq_length, hidden_size)\n",
        "        return self.out_linear(output)  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "class SwitchTransformer(nn.Module):\n",
        "    def __init__(self, num_experts, hidden_size, num_heads):\n",
        "        super(SwitchTransformer, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # List of expert layers (using attention)\n",
        "        self.experts = nn.ModuleList([MultiHeadSelfAttention(hidden_size, num_heads) for _ in range(num_experts)])\n",
        "\n",
        "        # Gate mechanism\n",
        "        self.gate = nn.Linear(hidden_size, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the gate values\n",
        "        gate_values = self.gate(x)  # (batch_size, seq_length, num_experts)\n",
        "\n",
        "        # Apply softmax to get the probability distribution over experts\n",
        "        gate_probs = torch.softmax(gate_values, dim=-1)  # (batch_size, seq_length, num_experts)\n",
        "\n",
        "        # Compute outputs from each expert\n",
        "        expert_outputs = [expert(x) for expert in self.experts]  # Each has shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Stack expert outputs to shape (num_experts, batch_size, seq_length, hidden_size)\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=0)  # (num_experts, batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Reshape gate_probs to (batch_size, seq_length, num_experts, 1) for broadcasting\n",
        "        gate_probs = gate_probs.unsqueeze(-1)  # (batch_size, seq_length, num_experts, 1)\n",
        "\n",
        "        # Use torch.einsum for weighted sum across the experts\n",
        "        output = torch.einsum('bsne,nesh->bsh', gate_probs, expert_outputs)  # (batch_size, seq_length, hidden_size)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    hidden_size = 768  # Example hidden size (e.g., BERT base)\n",
        "    num_experts = 4\n",
        "    num_heads = 8  # Number of attention heads\n",
        "    batch_size = 2\n",
        "    seq_length = 10\n",
        "\n",
        "    model = SwitchTransformer(num_experts, hidden_size=hidden_size, num_heads=num_heads)\n",
        "\n",
        "    # Dummy input (batch_size x seq_length x hidden_size)\n",
        "    input_tensor = torch.rand(batch_size, seq_length, hidden_size)\n",
        "    output = model(input_tensor)\n",
        "    print(\"Output shape:\", output.shape)  # Should be (batch_size, seq_length, hidden_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwB-_Q5-AZyj",
        "outputId": "6f546a5c-7ff5-448d-e95d-8a7e76e3cd67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_size // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == hidden_size\n",
        "        ), \"hidden_size must be divisible by num_heads\"\n",
        "\n",
        "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        queries = self.q_linear(x)\n",
        "        keys = self.k_linear(x)\n",
        "        values = self.v_linear(x)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, values)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
        "        return self.out_linear(output)\n",
        "\n",
        "class SwitchTransformer(nn.Module):\n",
        "    def __init__(self, num_experts, hidden_size, num_heads):\n",
        "        super(SwitchTransformer, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # List of expert layers (using attention)\n",
        "        self.experts = nn.ModuleList([MultiHeadSelfAttention(hidden_size, num_heads) for _ in range(num_experts)])\n",
        "\n",
        "        # Gate mechanism\n",
        "        self.gate = nn.Linear(hidden_size, num_experts)\n",
        "        self.output_layer = nn.Linear(hidden_size, tokenizer.vocab_size)  # Output layer to map to vocab size\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_values = self.gate(x)  # (batch_size, seq_length, num_experts)\n",
        "        gate_probs = torch.softmax(gate_values, dim=-1)  # (batch_size, seq_length, num_experts)\n",
        "\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=0)  # (num_experts, batch_size, seq_length, hidden_size)\n",
        "\n",
        "        gate_probs = gate_probs.unsqueeze(-1)  # (batch_size, seq_length, num_experts, 1)\n",
        "        output = torch.einsum('bsne,nesh->bsh', gate_probs, expert_outputs)  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Map output to token IDs\n",
        "        token_logits = self.output_layer(output)  # (batch_size, seq_length, vocab_size)\n",
        "        return token_logits\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    transformer_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    hidden_size = 768  # Example hidden size (e.g., BERT base)\n",
        "    num_experts = 4\n",
        "    num_heads = 8  # Number of attention heads\n",
        "    model = SwitchTransformer(num_experts, hidden_size=hidden_size, num_heads=num_heads)\n",
        "\n",
        "    input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = transformer_model(**tokens).last_hidden_state  # (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    # Pass the embeddings through the SwitchTransformer\n",
        "    output = model(embeddings)\n",
        "\n",
        "    # Convert the output logits to token IDs\n",
        "    decoded_ids = torch.argmax(output, dim=-1)  # Get the most likely token IDs\n",
        "    decoded_output = tokenizer.decode(decoded_ids[0].tolist(), skip_special_tokens=True)\n",
        "\n",
        "    print(\"Output shape:\", output.shape)  # Should be (batch_size, seq_length, vocab_size)\n",
        "    print(\"Decoded output:\", decoded_output)  # Should produce more meaningful text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlufaix-_mwT",
        "outputId": "f846db19-d613-4599-f083-3b94777d4629"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 12, 30522])\n",
            "Decoded output: usa articles usa usa usa usa articles articles usa usa regiment regiment\n"
          ]
        }
      ]
    }
  ]
}